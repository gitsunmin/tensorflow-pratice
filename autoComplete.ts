import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-node';
import { goodsNameList } from './goodsNameList';

const tokenize = (data: string[]) => {
    const charTokenizer = new Map<string, number>();
    const reverseCharTokenizer = new Map();
    let charIndex = 1;

    // ğŸ”¹ ê¸€ì ë‹¨ìœ„ë¡œ ë§¤í•‘
    for (const name of goodsNameList) {
        for (const char of name) {  // âœ¨ ë‹¨ì–´ê°€ ì•„ë‹ˆë¼ ê¸€ì ë‹¨ìœ„ë¡œ ë°˜ë³µ
            if (!charTokenizer.has(char)) {
                charTokenizer.set(char, charIndex);
                reverseCharTokenizer.set(charIndex, char);
                charIndex++;
            }
        }
    }
    return {
        charTokenizer,
        reverseCharTokenizer
    }
};

const textToCharSequence = (text: string, charTokenizer: Map<string, number>) => {
    return text.split("").map(char => charTokenizer.get(char) || 0);
}

const createTrainingData = (data: string[], charTokenizer: Map<string, number>) => {
    let inputSequences: number[][] = [];
    let outputChars: number[] = [];

    for (const name of data) {
        let tokenList = textToCharSequence(name, charTokenizer);
        for (let i = 1; i < tokenList.length; i++) {
            inputSequences.push(tokenList.slice(0, i));
            outputChars.push(tokenList[i]);
        }
    }

    return { inputSequences, outputChars };
};

const applyPadding = (inputSequences: number[][], maxLen: number) => {
    return inputSequences.map(seq => {
        const padding = Array(maxLen - seq.length).fill(0);
        return [...padding, ...seq];
    });
};

const convertToTensor = (inputSequences: number[][], outputChars: number[], maxLen: number) => {
    const XArray = applyPadding(inputSequences, maxLen);
    const X = tf.tensor2d(XArray, [XArray.length, maxLen], 'float32');
    const y = tf.tensor1d(outputChars, 'float32');

    return { X, y };
};

const createModel = (vocabSize: number, maxLen: number) => {
    const model = tf.sequential();

    model.add(tf.layers.embedding({
        inputDim: vocabSize,
        outputDim: 16,  // ê¸°ì¡´ 10 â†’ 16ìœ¼ë¡œ ì¦ê°€
        inputLength: maxLen
    }));

    model.add(tf.layers.lstm({ units: 128, returnSequences: true })); // ê¸°ì¡´ 50 â†’ 128 ìœ ë‹› ì¦ê°€
    model.add(tf.layers.lstm({ units: 64, returnSequences: false })); // ì¶”ê°€ LSTM ë ˆì´ì–´

    model.add(tf.layers.dense({ units: vocabSize, activation: 'softmax' }));

    return model;
};

const trainModel = async (model: tf.Sequential, X: tf.Tensor, y: tf.Tensor) => {
    model.compile({
        optimizer: tf.train.adam(0.005),  // ê¸°ì¡´ë³´ë‹¤ 2ë°° ì¦ê°€ (ê¸°ë³¸ê°’: 0.001)
        loss: 'sparseCategoricalCrossentropy',
        metrics: ['accuracy']
    });

    console.log("ğŸ“Œ ëª¨ë¸ í•™ìŠµ ì‹œì‘...");
    await model.fit(X, y, {
        epochs: 200,
        batchSize: 8,
        callbacks: {
            onEpochEnd: (epoch, logs) => {
                if (logs) {
                    console.log(`Epoch ${epoch + 1}: loss=${logs.loss}, accuracy=${logs.acc}`);
                }
            }
        }
    });

    console.log("ğŸ“Œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!");
};

const predictNextChar = (model: tf.Sequential, seedText: string, charTokenizer: Map<string, number>, reverseCharTokenizer: Map<number, string>, maxLen: number) => {
    let inputSeq = textToCharSequence(seedText, charTokenizer);
    const padding = Array(maxLen - inputSeq.length).fill(0);
    const paddedInput = [...padding, ...inputSeq];

    const inputTensor = tf.tensor2d([paddedInput], [1, maxLen], 'float32');
    const prediction = model.predict(inputTensor) as tf.Tensor;
    // ğŸ”¹ ì˜ˆì¸¡ëœ í™•ë¥ ê°’ í™•ì¸
    console.log("ğŸ“Œ ì˜ˆì¸¡ëœ í™•ë¥  ë¶„í¬:", prediction.arraySync());
    // ğŸ”¹ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ì¸ë±ìŠ¤ ì°¾ê¸°
    const predictedIndex = prediction.argMax(1).dataSync()[0];

    return reverseCharTokenizer.get(predictedIndex) || "ì˜ˆì¸¡ ì‹¤íŒ¨";
};

// ğŸ”¹ í† í°í™” ìˆ˜í–‰
const { charTokenizer, reverseCharTokenizer } = tokenize(goodsNameList);
const vocabSize = charTokenizer.size + 1;

// ğŸ”¹ í•™ìŠµ ë°ì´í„° ìƒì„±
const { inputSequences, outputChars } = createTrainingData(goodsNameList, charTokenizer);
const maxLen = inputSequences.reduce((max, seq) => Math.max(max, seq.length), 0);
// ğŸ”¹ íŒ¨ë”© ë° í…ì„œ ë³€í™˜
const { X, y } = convertToTensor(inputSequences, outputChars, maxLen);

// ğŸ”¹ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
const model = createModel(vocabSize, maxLen);
await trainModel(model, X, y);

// ğŸ”¹ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
console.log("ì˜ˆì¸¡ ê²°ê³¼:", predictNextChar(model, "ìœ ê¸°", charTokenizer, reverseCharTokenizer, maxLen));
console.log("ì˜ˆì¸¡ ê²°ê³¼:", predictNextChar(model, "êµ­ë‚´", charTokenizer, reverseCharTokenizer, maxLen));
console.log("ì˜ˆì¸¡ ê²°ê³¼:", predictNextChar(model, "ì˜¤ë©”", charTokenizer, reverseCharTokenizer, maxLen));